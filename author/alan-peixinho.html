<!DOCTYPE html>
<html lang="pt-br">
<head>

    <meta charset="utf-8">
    <title>Espaço do Peixinho <small>Alan Peixinho</small></title>
    <meta name="description" content="">
    <meta name="author" content="Alan Peixinho">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
    <script src="https://alanpeixinho.github.io/theme/html5.js"></script>
    <![endif]-->


    <!-- Le styles -->
    <link href="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.1.1/css/bootstrap.no-icons.min.css" rel="stylesheet">
    <link href="https://alanpeixinho.github.io/theme/local.css" rel="stylesheet">
    <link href="https://alanpeixinho.github.io/theme/pygments.css" rel="stylesheet">
    <link href="https://alanpeixinho.github.io/theme/font-awesome.css" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Gudea:400,400italic|Alegreya+SC' rel='stylesheet' type='text/css'>

</head>

<body>
<header class="blog-header">
  <div class="container">
    <div class="row-fluid">
      <div class="span9">
	<a href="https://alanpeixinho.github.io" class="brand">Espaço do Peixinho</a>
      </div>

      <div class="span3" id="blog-nav">
	<ul class="nav nav-pills pull-right">
	    <li >
	      <a href="https://alanpeixinho.github.io/category/posts.html ">posts</a>
	</ul>
      </div>
    </div> <!-- End of fluid row-->
  </div>   <!-- End of Container-->
</header>
    
<div class="container">
    <div class="content">
    <div class="row-fluid">

        <div class="span10">
        

        

    <div class='row-fluid''>
        <div class="article-title span9">
            <a href="https://alanpeixinho.github.io/sobre-classificadores-e-teorema-de-bayes.html"><h1>Sobre Classificadores e Teorema de Bayes</h1></a>
        </div>
    </div>
    <div class="row-fluid">
      <div class="span2">
<p>Wed 05 February 2020 </p>

<p style="text-align: left;">
Filed under <a href="https://alanpeixinho.github.io/category/posts.html">posts</a>
</p>
<p style="text-align: left;">
 
    Tags <a href="https://alanpeixinho.github.io/tag/machine-learning.html">machine learning</a> <a href="https://alanpeixinho.github.io/tag/data-science.html">data science</a> <a href="https://alanpeixinho.github.io/tag/chapel.html">chapel</a> </p>
<p>
</p>
      </div>
      <div class="article-content span8">
	<p>Umas das principais ferramentas na nossa caixa de ferramentas de reconhecimento de padrões certamente vem dos <strong>classificadores de padrões</strong>.</p>
<p>Então para começarmos nossa jornada, vamos entender alguns conceitos fundamentais.</p>
<p><a href="https://www.youtube.com/watch?v=cGufy1PAeTU"><img alt="" src="/images/zelda_bayes.png"></a></p>
<h2>Classificação de Padrões</h2>
<p>Imagine um cenário onde temos diversas amostras separadas em diferentes grupos.</p>
<p><img alt="" src="/images/pokemon_starters.png"></p>
<p>De forma bastante simples, qualquer pessoa mesmo que não conheça pokémon, é capaz de encontrar algum padrão na organização desses grupos. E, com isso, incluir uma nova amostra em um desses grupos.</p>
<p><img alt="" src="/images/poliwag.png"></p>
<p>Esse processo de identificar um padrão, e aprender a organizar novas amostras, seguindo o padrão pre-estabelecido é conhecido como <strong>classificação de padrões</strong>.</p>
<p>De forma mais simples, dado um conjunto de amostras que representem um problema que queremos aprender, as quais vamos chamar de <strong>base de treinamento</strong>, e aplicar essa mesma regra de organização para qualquer outra amostra nova que também pertença a essa população, num processo conhecido como <strong>predição</strong>.</p>
<blockquote>
<p>Agora como podemos criar uma forma automática capaz de encontrar padrões, aprender seu processo de construção, e aplicar essa regra a novas amostras desconhecidas?</p>
</blockquote>
<p><img alt="" src="/images/programmer_harry.jpg"></p>
<p>Magia arcana e matemática, são as principais abordagens utilizadas nessa tarefa. Devido a restrições de tempo, por hora vamos usar matemática para resolver esse problema.</p>
<p>Mais especificamente, vamos usar probabilidade, e um conceito conhecido como <a href="https://pt.wikipedia.org/wiki/Teorema_de_Bayes">Teorema de Bayes</a>.</p>
<h3>Base de treinamento</h3>
<p>Para os nossos exemplos, vamos trabalhar com a base de dados de classificação de flores <a href="https://pt.wikipedia.org/wiki/Conjunto_de_dados_flor_Iris">Iris</a>, onde dadas as informações de comprimento e largura das sépalas e pétalas, aprendemos a distinguir entre  <em>Iris setosa</em>, <em>Iris virginica</em> e <em>Iris versicolor</em>.</p>
<h3>Classificadores Probabilísticos</h3>
<p>O mesmo conceito pode ser descrito, ainda de forma bem superficial, com uma abordagem um pouco mais formal.
Afinal, o que é a nossa amostra?
Para propositos práticos, vamos representar nossas amostras matematicamente como um vetor de características numericas (ao menos por enquanto). Dessa forma cada amostra \(x\) é representada como um ponto na dimensão \(R^n\), ou seja, \(x\in\mathbb{R}^n\).
Cada uma dessas amostras, como comentamos anteriormente, é divida entre \(C=\{c_1,c_2,\dots,c_c\}\) classes distintas, onde um rótulo \(y\in C\) representa o grupo ao qual essa amostra pertence.</p>
<p>Agora, pensando numa abordagem probabilística, nosso classificador pode ser definido como a probabilidade </p>
<div class="math">$$ P(y=c_i|x) $$</div>
<p> para cada uma das classes em \(c\).</p>
<blockquote>
<p>Auxílio Alzheimer: P(A|B) significa a probabilidade condicional de A dado B.
Que nos dá a probabilidade de um evento A ocorrer, dado que um evento B já ocorreu.</p>
</blockquote>
<h3>Teorema de Bayes</h3>
<p>De forma muito simplificada, o teorema de Bayes nos permite calcular uma probabilidade condicionada a um evento, a partir de informações de probabilidade que conhecemos.</p>
<p>Ok, muito vago.</p>
<p>Então, vamos definí-lo de forma mais clara.</p>
<div class="math">$$ P(A|B) =  \frac{P(A) P(B|A)}{P(B)} $$</div>
<p>Basicamente, o teorema de Bayes nos permite calcular a probabilidade de um evento condicional, desde que conheçamos a probabilidade de cada evento isolado -&gt; \(P(A) \)e \(P(B)\), bem como a probabilidade da condicional inversa -&gt; \(P(B|A)\).</p>
<p>Num cenário de classificação de padrões, isso nos permite descrever a probabilidade de uma amostra pertencer a uma determinada classe \(c\), como:</p>
<div class="math">$$ P(y=c_i|x) = \frac{P(y=c_i) P(x|y=c_i)}{P(x)} $$</div>
<p>Isso significa que conseguimos construir um classificador probabilístico desde que possamos calcular os valores (ou uma aproximação) das probabilidades dos eventos base \(P(y=c_i),P(x)\ e\ P(x|y=c_i)\).</p>
<p>Agora que temos a estrutura necessária para o nosso classificador bayesiano, precisamos definir como computar as probabilidades para cada um dos eventos base.</p>
<h6>\( \mathbf{P(y=c_i)} \)</h6>
<p>A probabilidade de que caso selecionemos qualquer amostra aleatoriamente do nosso problema ela pertença a classe \(c_i\).
Considerando que nossa base de treinamento tenha sido construída de forma aleatória (o que é importante, mas comumente ignorado), podemos dizer que a probabilidade de tal evento pode ser aproximada pelo número de amostras que pertencem a classe \(c_i\), pelo número total de amostras da nossa base de treinamento.</p>
<div class="math">$$ P(y=c_i) = \frac{|y=c_i|}{|y|} $$</div>
<p>Achei fácil.</p>
<h6>\( \mathbf{P(x)} \)</h6>
<p>A probabilidade de ocorrencia da nossa amostra \(x\).
Agora temos algo um pouco mais complicado. pois \(x\in\mathbb{R}^n\), ou seja, além de um número real, temos n variaveis distintas a considerar.</p>
<p>Para tratar o caso de múltiplas variáveis, vamos introduzir uma suposição, a de que todas as características sao <strong>mutuamente exclusivas</strong>, dessa forma uma probabilidade \(P(x)\) corresponde ao produto das probabilidades de cada variável \(x_i\). Ou seja:</p>
<div class="math">$$P(x)=\prod{P(x_i)}$$</div>
<p>Contudo, ainda nos resta computar \(P(x_i)\) para cada variável do nosso vetor de características. E aí reside nosso maior problema. Para tal, teríamos que conhecer a distribuição de cada uma das variáveis do nosso vetor de características, as quais não conhecemos.</p>
<p>Como quase sempre em machine learning, a resposta é assumir que os dados seguem alguma distribuição (quase sempre uma <a href="https://pt.wikipedia.org/wiki/Distribui%C3%A7%C3%A3o_normal">distribuição normal</a>), fechar os olhos e seguir a vida. E, desde que essa nossa distribuição não se afaste (muito) da distribição real dos dados, o modelo consegue se virar razoavelmente bem.</p>
<blockquote>
<p>Mas tem um limite pro quanto você pode chutar o balde, seu preguiçoso. Então, no mínimo, de uma olhada nos seus dados pra saber se a distribuição que você está assumindo não está muito longe da distribuição real dos seus dados, antes de assumir que o pobre classificador bayesiano não funciona.</p>
</blockquote>
<p>Para os nossos dados da base de iris, vamos asumir uma distribuição normal, onde média e desvio padrão sao computadas a partir dos dados.</p>
<p><img alt="" src="/images/iris_feats_plot.svg"></p>
<p>Como podemos notar, embora a distribuição assumida (linhas) não apresente um casamento perfeito com os dados (barras), temos uma aproximação boa o suficiente para assumir, para nossos propósitos maléficos, que nossa distribuição está correta.</p>
<h4>\(\mathbf{P(x|y=c_i)}\)</h4>
<p>Valem as mesmas regras para o computo de \(P(x)\), contudo, nesse caso, vamos isolar apenas as amostras em que \(y=c_i\).</p>
<p>Com isso temos os conceitos básicos pra entender a base de um classificador bayesiano, bem como podemos entender as principais restrições na construção de seu modelo, que são a <strong>independencia entre as variáveis</strong> do nosso vetor de características, e que o modelo da *<em>função densidade probabilidade</em> reflita a distribuição real dos dados.</p>
<blockquote>
<p>É bastante importante que você leve em consideração as restrições de um modelo antes de utilizá-lo em qualquer problema de classificação. Dessa forma, você pode <strong>ignorar essas restrições de forma consciente</strong>  (como todo bom cientista de dados), e aplicar o modelo ainda assim, porque vai que funciona.</p>
</blockquote>
<h3>Implementando a p###a toda</h3>
<p>Considerando as definições utilizadas anteriormente, a maioria das implementações de classificadores terão uma mesma interface comum, tanto para o aprendizado quanto para a predição. Algo nas linhas de:</p>
<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="nx">X</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">m</span><span class="p">][</span><span class="mi">1</span><span class="o">..</span><span class="nx">n</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">,</span><span class="w"> </span><span class="nx">Y</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">m</span><span class="p">]</span><span class="w"> </span><span class="kt">int</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="o">..</span><span class="p">.</span>
<span class="p">}</span>
</code></pre></div>

<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="nx">x</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">):</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="o">..</span><span class="p">.</span>
<span class="p">}</span>
</code></pre></div>

<p>Com isso, podemos implementar os passos para o treinamento do nosso classificador.</p>
<blockquote>
<p>Eu sei que chapel nao é la uma linguagem muito conhecida, mas ela é legivel o suficiente, pra qualquer um que venha de Python ou C++.</p>
</blockquote>
<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">train</span><span class="p">(</span><span class="nx">X</span><span class="p">,</span><span class="w"> </span><span class="nx">Y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="kd">const</span><span class="w"> </span><span class="p">(</span><span class="nx">n_train</span><span class="p">,</span><span class="w"> </span><span class="nx">n_feats</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">X</span><span class="p">.</span><span class="nx">shape</span><span class="p">;</span>
<span class="w">  </span><span class="kd">const</span><span class="w"> </span><span class="nx">n_classes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">max</span><span class="p">(</span><span class="nx">Y</span><span class="p">):</span><span class="w"> </span><span class="kt">int</span><span class="p">(</span><span class="mi">64</span><span class="p">);</span>

<span class="w">  </span><span class="c1">//global feature model</span>
<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">mu</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_feats</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">;</span>
<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">sigma</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_feats</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">;</span>

<span class="w">  </span><span class="c1">//classes feature model</span>
<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">mu_c</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_feats</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">;</span>
<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">sigma_c</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_feats</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">;</span>

<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">p_class</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">;</span>
</code></pre></div>

<p>Essas sao as principais variaveis que vamos usar.
Onde <em>n_train</em>, <em>n_feats</em> e <em>n_classes</em> sao exatamente isso que elas descrevem, nem sei porque incluí isso, <em>mu</em> e <em>mu_c</em> representam o valor médio global das features e o valor médio das features para cada classe, da mesma forma, <em>sigma</em> e <em>sigma_c</em>, representam o desvio padrão global e das classe, <em>p_class</em> representa a probabilidade de uma amostra pertencer a uma determinada classe (\(P(y=c_i)\)).</p>
<div class="highlight"><pre><span></span><code><span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">n_train_c</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="p">]</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="w">  </span><span class="nx">mu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="nx">mu_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="nx">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="nx">sigma_c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>

<span class="w">  </span><span class="c1">//compute mean</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_train</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">Y</span><span class="p">(</span><span class="nx">i</span><span class="p">):</span><span class="w"> </span><span class="kt">int</span><span class="p">;</span>
<span class="w">    </span><span class="nx">mu</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">X</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span><span class="o">..</span><span class="p">);</span>
<span class="w">    </span><span class="nx">mu_c</span><span class="p">(</span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="nx">X</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">);</span>
<span class="w">    </span><span class="nx">n_train_c</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="nx">mu</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="nx">n_train</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">f</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_feats</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">mu_c</span><span class="p">(</span><span class="o">..</span><span class="p">,</span><span class="w"> </span><span class="nx">f</span><span class="p">)</span><span class="w"> </span><span class="o">/=</span><span class="w"> </span><span class="nx">n_train_c</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">//compute standard deviation</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_train</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">Y</span><span class="p">(</span><span class="nx">i</span><span class="p">):</span><span class="w"> </span><span class="kt">int</span><span class="p">;</span>
<span class="w">    </span><span class="nx">sigma</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="p">(</span><span class="nx">X</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">;</span>
<span class="w">    </span><span class="nx">sigma_c</span><span class="p">(</span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">)</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="p">(</span><span class="nx">X</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">mu_c</span><span class="p">(</span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="nx">sigma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">sqrt</span><span class="p">(</span><span class="nx">sigma</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">n_train</span><span class="p">);</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">f</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_feats</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">sigma_c</span><span class="p">(</span><span class="o">..</span><span class="p">,</span><span class="w"> </span><span class="nx">f</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">sqrt</span><span class="p">(</span><span class="nx">sigma_c</span><span class="p">(</span><span class="o">..</span><span class="p">,</span><span class="nx">f</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">n_train_c</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="c1">//compute P(class)</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="nx">p_class</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">n_train_c</span><span class="p">(</span><span class="nx">c</span><span class="p">):</span><span class="w"> </span><span class="kt">real</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nx">n_train</span><span class="p">;</span>
<span class="w">  </span><span class="p">}</span>
</code></pre></div>

<p>Com isso, já temos todas as informações necessárias ao nosso modelo.</p>
<p>Mas, na maioria das implementações você verá que trabalha-se com o log das probabilidades.
Pois assim, podemos substituir o produtório citado acima, por uma simples soma, já que:</p>
<div class="math">$$\prod{P(x_i)}=exp(\sum(log(x)))$$</div>
<p>, e o mesmo vale pra demais operacões de multiplicação ou divisão que são substituídas por uma simples soma/subtração.</p>
<p>Então, uma linha a mais:</p>
<div class="highlight"><pre><span></span><code><span class="nx">p_class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">log</span><span class="p">(</span><span class="nx">p_class</span><span class="p">);</span>
</code></pre></div>

<p>A predicão, agora, nada mais é do que, utilizarmos essas variaveis de modelo, para computar as probabilidades condicionais necessárias ao classificador bayesiano.</p>
<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">predict</span><span class="p">(</span><span class="nx">x</span><span class="p">):</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="c1">//P(x)</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">p_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">gaussianPdf</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">mu</span><span class="p">,</span><span class="w"> </span><span class="nx">sigma</span><span class="p">);</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">p_joint_data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">sum</span><span class="p">(</span><span class="nx">log</span><span class="p">(</span><span class="nx">p_data</span><span class="p">));</span>

<span class="w">    </span><span class="c1">//P(y|x)</span>
<span class="w">    </span><span class="kd">var</span><span class="w"> </span><span class="nx">p_class_given_data</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="p">]</span><span class="w"> </span><span class="kt">real</span><span class="p">;</span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="nx">c</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_classes</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">//P(x|y)</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="nx">p_data_given_class</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">gaussianPdf</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">mu_c</span><span class="p">(</span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">),</span><span class="w"> </span><span class="nx">sigma_c</span><span class="p">(</span><span class="nx">c</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">));</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="nx">p_joint_data_given_class</span><span class="w"> </span><span class="o">=</span><span class="w">  </span><span class="nx">sum</span><span class="p">(</span><span class="nx">log</span><span class="p">(</span><span class="nx">p_data_given_class</span><span class="p">));</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="nx">p</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">p_joint_data_given_class</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">p_class</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="nx">p_joint_data</span><span class="p">;</span>
<span class="w">      </span><span class="nx">p_class_given_data</span><span class="p">(</span><span class="nx">c</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">p</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>


<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="nx">argmax</span><span class="p">(</span><span class="nx">p_class_given_data</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>Após computado o vetor de probabilidades \(P(y=c_i|x)\), nossa classe predita corresponde a classe de maior probabilidade \(\arg\max(P(y=c_i))\).</p>
<p>E, como adotamos um modelo gaussiano, a implementação da função densidade probabilidade gaussiana:</p>
<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">gaussianPdf</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span><span class="w"> </span><span class="nx">mu</span><span class="p">,</span><span class="w"> </span><span class="nx">sigma</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kd">const</span><span class="w"> </span><span class="nx">expo</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="p">(</span><span class="nx">x</span><span class="o">-</span><span class="nx">mu</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">epsilon</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">sigma</span><span class="o">**</span><span class="mi">2</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">epsilon</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="nx">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">pi</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="nx">sigma</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">epsilon</span><span class="w"> </span><span class="p">)))</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="nx">exp</span><span class="p">(</span><span class="nx">expo</span><span class="p">);</span>
<span class="p">}</span>
</code></pre></div>

<p>Vale lembrar, que nesse caso, adotamos um modelo gaussiano para o nosso classificador, mas você pode adotar outras funções que descrevam melhor seu modelo. Mas aí, já é um problema seu.</p>
<p>Por fim, vamos colocar tudo pra rodar num exemplo.</p>
<p>Nossa base de dados de classificação de flores está num arquivo CSV, contendo os três descritores da amostra, e sua classe, respectivamente.</p>
<div class="highlight"><pre><span></span><code><span class="mf">5.30</span><span class="p">,</span><span class="mf">3.70</span><span class="p">,</span><span class="mf">1.50</span><span class="p">,</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">1.00</span>
<span class="mf">5.00</span><span class="p">,</span><span class="mf">3.30</span><span class="p">,</span><span class="mf">1.40</span><span class="p">,</span><span class="mf">0.20</span><span class="p">,</span><span class="mf">1.00</span>
<span class="mf">7.00</span><span class="p">,</span><span class="mf">3.20</span><span class="p">,</span><span class="mf">4.70</span><span class="p">,</span><span class="mf">1.40</span><span class="p">,</span><span class="mf">2.00</span>
<span class="mf">6.40</span><span class="p">,</span><span class="mf">3.20</span><span class="p">,</span><span class="mf">4.50</span><span class="p">,</span><span class="mf">1.50</span><span class="p">,</span><span class="mf">2.00</span>
</code></pre></div>

<p>Para testar nosso classificador bayesiano, vamos dividir aleatoriamente esse conjunto de dados em 2, chamados de treino e teste.
O conjunto de treino será utilizado no treinamento do nosso classificador, e o conjunto de testes será usado para verificarmos o grau de acerto do nosso classificador, comparando a classe predita, com a classe real da amostra.</p>
<div class="highlight"><pre><span></span><code><span class="k">use</span><span class="w"> </span><span class="nx">data</span><span class="p">;</span>
<span class="k">use</span><span class="w"> </span><span class="nx">naive_bayes</span><span class="p">;</span>
<span class="k">use</span><span class="w"> </span><span class="nx">math</span><span class="p">;</span>
<span class="k">use</span><span class="w"> </span><span class="nx">utils</span><span class="p">;</span>

<span class="kd">config</span><span class="w"> </span><span class="kd">const</span><span class="w"> </span><span class="nx">csv_dataset</span><span class="p">:</span><span class="w"> </span><span class="kt">string</span><span class="p">;</span>

<span class="k">proc</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">try</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="nx">dataset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">data</span><span class="p">.</span><span class="nx">readCSV</span><span class="p">(</span><span class="nx">csv_dataset</span><span class="p">,</span><span class="w"> </span><span class="nx">delimiter</span><span class="o">=</span><span class="s">&#39;,&#39;</span><span class="p">);</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="p">(</span><span class="nx">Xtrain</span><span class="p">,</span><span class="w"> </span><span class="nx">Ytrain</span><span class="p">,</span><span class="w"> </span><span class="nx">Xtest</span><span class="p">,</span><span class="w"> </span><span class="nx">Ytest</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">dataset</span><span class="p">.</span><span class="nx">split</span><span class="p">(</span><span class="nx">trainRatio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="nx">clf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">naive_bayes</span><span class="p">.</span><span class="nx">train</span><span class="p">(</span><span class="nx">Xtrain</span><span class="p">,</span><span class="w"> </span><span class="nx">Ytrain</span><span class="p">);</span>
<span class="w">      </span><span class="kd">var</span><span class="w"> </span><span class="nx">Ypred</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">Ytest</span><span class="p">.</span><span class="nx">size</span><span class="p">]</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
<span class="w">      </span><span class="nx">Ypred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">clf</span><span class="p">.</span><span class="nx">predict_batch</span><span class="p">(</span><span class="nx">Xtest</span><span class="p">);</span>

<span class="w">      </span><span class="nx">writeln</span><span class="p">(</span><span class="s">&quot;Score: %4.2dr&quot;</span><span class="p">.</span><span class="nx">format</span><span class="p">(</span><span class="nx">accuracy</span><span class="p">(</span><span class="nx">Ytest</span><span class="p">,</span><span class="w"> </span><span class="nx">Ypred</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mf">100.0</span><span class="p">));</span>

<span class="w">    </span><span class="p">}</span><span class="w"> </span><span class="k">catch</span><span class="w"> </span><span class="nx">e</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nx">writeln</span><span class="p">(</span><span class="s">&quot;deu ruim &quot;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nx">e</span><span class="p">:</span><span class="kt">string</span><span class="p">);</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</code></pre></div>

<p>Boa parte desse código é composto apenas de operações chatas como ler arquivos com a base de dados, dividí-lo aleatóriamente em conjunto de treino e testes, e outras coisas mundanas. Fora isso, incluímos dois pontos mais importantes que são, a predição em lote e o cálculo de acurácia (grau de acerto do nosso classificador).</p>
<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">predict_batch</span><span class="p">(</span><span class="nx">X</span><span class="p">):</span><span class="w"> </span><span class="p">[]</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="kd">const</span><span class="w"> </span><span class="nx">n_test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">X</span><span class="p">.</span><span class="nx">shape</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">Y</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="o">..</span><span class="nx">n_test</span><span class="p">]</span><span class="w"> </span><span class="kt">int</span><span class="p">;</span>
<span class="w">  </span><span class="k">forall</span><span class="w"> </span><span class="nx">i</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="mi">1</span><span class="o">..</span><span class="nx">n_test</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nx">Y</span><span class="p">(</span><span class="nx">i</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">predict</span><span class="p">(</span><span class="nx">X</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span><span class="w"> </span><span class="o">..</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nx">Y</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>A predição em batch, nada mais é do que uma função que calcula a predição do nosso modelo, para um conjunto de amostras, onde podemos aproveitar a natureza independente das amostras para executar cada predição de forma paralela.</p>
<div class="highlight"><pre><span></span><code><span class="k">proc</span><span class="w"> </span><span class="nf">accuracy</span><span class="p">(</span><span class="nx">X1</span><span class="p">,</span><span class="w"> </span><span class="nx">X2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kd">var</span><span class="w"> </span><span class="nx">correct</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.0</span><span class="p">;</span>
<span class="w">  </span><span class="kd">const</span><span class="w"> </span><span class="nx">total</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nx">X1</span><span class="p">.</span><span class="nx">size</span><span class="p">;</span>

<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="nx">x1</span><span class="p">,</span><span class="w"> </span><span class="nx">x2</span><span class="p">)</span><span class="w"> </span><span class="kd">in</span><span class="w"> </span><span class="k">zip</span><span class="p">(</span><span class="nx">X1</span><span class="p">,</span><span class="w"> </span><span class="nx">X2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="nx">x1</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nx">x2</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="nx">correct</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="p">}</span>
<span class="w">  </span><span class="p">}</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="nx">correct</span><span class="o">/</span><span class="nx">total</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div>

<p>E a acurácia, que nada mais é do que a mais simples medida de acerto que podemos usar para avaliar nosso classificador: a proporção entre amostras corretamente classificadas e o número total de amostras do nosso conjunto de testes.</p>
<p>Rodando pra checar os resultados, temos ...</p>
<div class="highlight"><pre><span></span><code>./run_naive_bayes<span class="w"> </span>--csv_dataset<span class="w"> </span>./data/digits.csv<span class="w">                                                                                     </span>
Score:<span class="w"> </span><span class="m">90</span>.32
</code></pre></div>

<p>Como nosso classificador "aprende" com os dados de entrada, podemos notar que, diferentes conjuntos de treinamento podem fazer com que nosso classificador aprenda modelos com diferentes taxas de acerto.</p>
<p>Então, rodando de novo ...</p>
<div class="highlight"><pre><span></span><code>./run_naive_bayes<span class="w"> </span>--csv_dataset<span class="w"> </span>./data/digits.csv<span class="w">                                                                                     </span>
Score:<span class="w"> </span><span class="m">90</span>.77
</code></pre></div>

<p>Por isso, usualmente, para avaliarmos nosso modelo, é comum que calculemos a média de várias rodadas de treino e teste, e assim conseguirmos uma melhor avaliação do nosso modelo.</p>
<p>Mas, basicamente, é isso aí.
Com esses conceitos basicos, você consegue implementar seu próprio classificador, e entender um pouco das entranhas de como funciona um modelo de aprendizado de máquina.</p>
<p>Se quiser rodar o código completo, ele está disponível <a href="https://github.com/alanpeixinho/FishLearning">aqui</a>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script> 
	<a class="btn btn-mini xsmall" href="https://alanpeixinho.github.io/sobre-classificadores-e-teorema-de-bayes.html">
          <i class="icon-comment"></i> Comment </a>
	<hr />
      </div>
      
    </div>
    
<div class="pagination">
<ul>
    <li class="prev disabled"><a href="#">&larr; Previous</a></li>

    <li class="active"><a href="https://alanpeixinho.github.io/author/alan-peixinho.html">1</a></li>

    <li class="next disabled"><a href="#">&rarr; Next</a></li>

</ul>
</div>

 
  
        </div>
        
        
    </div>     </div> </div>

<!--footer-->
<div class="container">
  <div class="well" style="background-color: #E9EFF6">
    <div id="blog-footer">
      <div class="row-fluid">
	<div class="social span2" align="center" id="socialist">
	  <ul class="nav nav-list">
	    <li class="nav-header">
	      Social
	    </li>
	    <li><a href="http://github.com/alanpeixinho"><i class="icon-github" style="color: #1f334b"></i>github</a></li>
	    <li><a href="https://br.linkedin.com/in/alan-zanoni-peixinho-2816ab23"><i class="icon-linkedin" style="color: #1f334b"></i>linkedin</a></li>

	  </ul>
	</div>
        <div class="links span2" align="center">
          <ul class="nav nav-list">
            <li class="nav-header"> 
              Links
            </li>
            
            <li><a href="https://github.com/alanpeixinho/FishLearning">Fish Learning</a></li>
          </ul>
        </div>
	<div class="site-nav span2" align="center">
          <ul class="nav nav-list" id="site-links">
            <li class="nav-header"> 
              Site
            </li>
            <li><a href="https://alanpeixinho.github.io"><i class="icon-home" style="color: #1f334b">
                </i>Home</a></li>
            <li><a href="https://alanpeixinho.github.io/archives.html"><i class="icon-list" style="color: #1f334b">
                </i>Archives</a></li>
	    <li><a href="https://alanpeixinho.github.io/tags.html"><i class="icon-tags" style="color: #1f334b">
                </i>Tags</a></li>
	    
	  </ul>

        </div>

      </div> <!--end of fluid row-->
    </div> <!--end of blog-footer-->
    <hr />
    <p align="center"><a href="https://alanpeixinho.github.io">Espaço do Peixinho</a>
      &copy; Alan Peixinho
    Powered by <a href="https://github.com/getpelican/pelican">Pelican</a> and
        <a href="https://twitter.github.com/bootstrap">Twitter Bootstrap</a>. 
        Icons by <a href="https://fortawesome.github.com/Font-Awesome">Font Awesome</a> and 
        <a href="https://gregoryloucas.github.com/Font-Awesome-More">Font Awesome More</a></p>

  </div> <!--end of well -->
</div> <!--end of container -->

<!--/footer-->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.8.3/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/twitter-bootstrap/2.2.2/js/bootstrap.min.js"></script>



</body>
</html>